# -*- coding: utf-8 -*-
"""ml_house_price_pred.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1l3msgUnibN04tODlLJAPd2U0EIuySkXk

# DAY 1: Download Dataset & Primary Analysis
"""

#download data from kaggle
!pip install opendatasets

import pandas as pd
import numpy as np
import opendatasets as od
import seaborn as sns
import matplotlib.pyplot as plt
import sklearn

# imputing missing values in num_cols
from sklearn.impute import SimpleImputer
# scaling numeric value in 1-0 range
from sklearn.preprocessing import MinMaxScaler,FunctionTransformer
# Combine the numerical and categorical pipelines
from sklearn.compose import ColumnTransformer

od.download("https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques/data")
#hemantk777  721b095dacb7cfa6129768282921b8fb

raw_data=pd.read_csv("/content/house-prices-advanced-regression-techniques/train.csv")
test_data=pd.read_csv("/content/house-prices-advanced-regression-techniques/test.csv")

raw_data.head()

test_data.head(2)

raw_data.shape

raw_data.info()

raw_data.describe()

raw_data.columns

correlation=raw_data.select_dtypes(include=['number']).corr()
correlation.head()



sns.histplot(raw_data['SalePrice'], kde=True)
plt.title("Distribution of Sale Prices")
###plt.savefig("Distributions of Sale Prices", dpi=300, bbox_inches='tight')                                         # fixes subplot spacing
plt.show()

# checking missing values
missing_value=raw_data.isna().sum().sort_values(ascending=False).head(20)
#missing_values=[i for i in missing_value]
missing_value

# See percentage of missing data
((raw_data.isnull().sum() / len(raw_data)) * 100).sort_values(ascending=False).head(10).reset_index()

"""# DAY 2: Exploratory Data Analysis (EDA)"""

#   numeric dataframe and cols   &   catagorical dataframe and cols
num_df=raw_data.select_dtypes(include=['int64','float64'])
num_cols=num_df.columns.tolist()

cat_df=raw_data.select_dtypes(exclude=['int64','float64'])
cat_cols=cat_df.columns.tolist()

num_df.shape[1]

num_df.hist(
    figsize=(15, 12),
    bins=30,
    color='steelblue',          # change bar colour
    edgecolor='black',
    sharex=False, sharey=False  # independent axes
)
plt.suptitle("Distributions of Numeric Features", y=1.02)  # y moves title up
plt.tight_layout()
###plt.savefig("Distributions of Numeric Features", dpi=300, bbox_inches='tight')                                         # fixes subplot spacing
plt.show()

import os

# Create folder to save plots
###os.makedirs("cat_charts", exist_ok=True)

# Loop through each categorical column
for col in cat_df.columns:
    plt.figure(figsize=(5, 3))
    sns.countplot(x=col, data=raw_data, order=raw_data[col].value_counts().index)
    plt.xticks(rotation=45)
    plt.title(f'Count plot of {col}')
    plt.tight_layout()

    # Save the figure
    ###filename = f"cat_charts/{col}_countplot.png"
    ###plt.savefig(filename, dpi=300, bbox_inches='tight')

    # ✅ Show the plot in the notebook
    plt.show()

"""## Download a Whole Folder (cat_ chart) from Colab

"""

#import shutil
# Zip the entire folder (cat_charts becomes cat_charts.zip)
#shutil.make_archive("cat_charts", 'zip', "cat_charts")



# Check correlation with target
correlation_with_target=correlation.corr()['SalePrice'].sort_values(ascending=False).head(15)
correlation_with_target

# Visualize correlations
plt.figure(figsize=(10, 8))
sns.heatmap(raw_data[correlation_with_target.index].corr(), annot=True, cmap='coolwarm')
plt.title("Top Feature Correlations with SalePrice")
plt.figure(figsize=(7, 3))
###plt.savefig("Top Feature Correlations with SalePrice", dpi=300, bbox_inches='tight')
plt.show()

sns.scatterplot(x='GrLivArea', y='SalePrice', data=raw_data)
plt.title("Living Area vs Sale Price")
###plt.savefig("Living Area vs Sale Price", dpi=300, bbox_inches='tight')
plt.show()

sns.boxplot(x='OverallQual', y='SalePrice', data=raw_data)
plt.title("overall quality vs sale price")
###plt.savefig("overall quality vs sale price1", dpi=300, bbox_inches='tight')
plt.show()

# Sort largest living areas to see if prices are aligned
raw_data.sort_values('GrLivArea', ascending=False)[['GrLivArea', 'SalePrice']].head(10)

"""# Day 3: Data Cleaning & Preprocessing"""

raw_data.isnull().sum().sort_values(ascending=False).head(15)

#Drop Columns with Too Many Missing Values  Columns like [PoolQC, Alley, Fence, MiscFeature] have too many NAs.
raw_data=raw_data.drop(columns=["Alley", "PoolQC", "Fence", "MiscFeature", "FireplaceQu","MasVnrType"]).copy()

raw_data.shape

# imputing / filling missing values in num_cols
from sklearn.impute import SimpleImputer
imputer=SimpleImputer(strategy='median').fit(raw_data[num_cols])
raw_data[num_cols]=imputer.transform(raw_data[num_cols])

raw_data[num_cols].isna().sum()

# for removing cols name i.e already drop in prev step['Alley', 'MasVnrType', 'FireplaceQu', 'PoolQC', 'Fence', 'MiscFeature']

to_remove = ['Alley', 'MasVnrType', 'FireplaceQu', 'PoolQC', 'Fence', 'MiscFeature']
cat_cols = [col for col in cat_cols if col not in to_remove]
cat_cols

## imputing / filling missing values in cat_cols
raw_data[cat_cols] = raw_data[cat_cols].fillna(raw_data[cat_cols].mode().iloc[0])

# encode categorical cols in one hot encoder
from sklearn.preprocessing import OneHotEncoder
encoder=OneHotEncoder(sparse_output=False,handle_unknown='ignore').fit(raw_data[cat_cols])
encoded_cols=list(encoder.get_feature_names_out(cat_cols))    # cols name for encoded cols
raw_data[encoded_cols]=encoder.transform(raw_data[cat_cols])

raw_data[encoded_cols].shape

#combine num and cat cols
raw_data_new=raw_data[num_cols + encoded_cols]
raw_data_new

#Check Skewness & Fix It (Shrinks big numbers so outliers don’t dominate)
from scipy.stats import skew
import numpy as np

numeric_feats = raw_data_new.select_dtypes(include=['int64', 'float64']).columns
skewed_feats = raw_data_new[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)
high_skew = skewed_feats[skewed_feats > 0.75]

raw_data_new[high_skew.index] = np.log1p(raw_data[high_skew.index])

raw_data[high_skew.index].head(2)

#  remove hasgtag for file saving
# raw_data.to_csv("cleaned_train.csv", index=False)

"""# Day 4 – Model Training & Evaluation"""

cat_df.shape

x=raw_data_new.drop("SalePrice",axis=1)
y=raw_data_new['SalePrice']
y

# train test split
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

#                    LinearRegression
from sklearn.linear_model import LinearRegression
lr= LinearRegression()
lr.fit(x_train,y_train)

#                    RandomForestRegressor

from sklearn.ensemble import RandomForestRegressor
rf = RandomForestRegressor(n_estimators=100, random_state=42)
rf.fit(x_train, y_train)

#                    XGBRegressor
from xgboost import XGBRegressor
xgb = XGBRegressor(n_estimators=100, learning_rate=0.1)
xgb.fit(x_train, y_train)

#Evaluate Models
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np

def evaluate(model, x_test, y_test):
    preds = model.predict(x_test)
    mae = mean_absolute_error(y_test, preds)
    rmse = np.sqrt(mean_squared_error(y_test, preds))
    r2 = r2_score(y_test, preds)
    return mae, rmse, r2

print("Linear Regression:", evaluate(lr, x_test, y_test))
print("Random Forest:", evaluate(rf, x_test, y_test))
print("XGBoost:", evaluate(xgb, x_test, y_test))

#Save the Best Model
import joblib
joblib.dump(xgb, "xgb_model.pkl")

#Plot Prediction vs Actual
import matplotlib.pyplot as plt

plt.scatter(y_test, xgb.predict(x_test))
plt.xlabel("Actual Prices")
plt.ylabel("Predicted Prices")
plt.title("XGBoost Predictions")
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], "r--")
###plt.savefig("plot Prediction vs Actual", dpi=300, bbox_inches='tight')
plt.show()

"""# Day 5 – Advanced ML Concepts & Interpretability

## Model Explainability with SHAP & LIME
 *Use for model transparency and stakeholder trust.*
"""

#     Model Explainability with SHAP & LIME
import shap
explainer = shap.Explainer(xgb)          # Use trained model
shap_values = explainer(x_test)

shap.plots.backend = "matplotlib"   # Use matplotlib backend for plots

shap.plots.beeswarm(shap_values, show=False) # Global importance
plt.title("SHAP Global Feature Importance")
plt.tight_layout()
plt.savefig("shap_beeswarm_plot.png", dpi=300, bbox_inches='tight')
plt.show()

shap.plots.waterfall(shap_values[0], show=False)  # Single prediction breakdown
plt.title("SHAP Waterfall for 1 Prediction")
plt.tight_layout()
plt.savefig("shap_waterfall_plot.png", dpi=300, bbox_inches='tight')
plt.show()

!pip install lime

from lime.lime_tabular import LimeTabularExplainer

explainer = LimeTabularExplainer(
    training_data=x_train.values,
    feature_names=x_train.columns,
    mode='regression'
)

exp = explainer.explain_instance(x_test.iloc[0].values, xgb.predict)
exp.show_in_notebook()

"""## 2. Feature Drift / Data Drift (Basics)
| Type          | Description                            | Tool                   |
| ------------- | -------------------------------------- | ---------------------- |
| Feature Drift | Feature distribution changes over time | `evidently`, `whylogs` |
| Data Drift    | Overall change in data pattern         | `scipy.stats.ks_2samp` |

Use this to monitor real-world performance.
"""

# Simulate X_new (for practice)
import pandas as pd
import numpy as np

# Simulate new data by adding noise to training data
x_new = x_train.copy()
x_new["GrLivArea"] = x_new["GrLivArea"] * np.random.normal(1.1, 0.1, size=len(x_new))

from scipy.stats import ks_2samp

# Compare training vs new production feature
ks_2samp(x_train["GrLivArea"], x_new["GrLivArea"])

from scipy.stats import ks_2samp

stat, p = ks_2samp(x_train["GrLivArea"], x_new["GrLivArea"])
print(f"p-value: {p}")

if p < 0.05:
    print("⚠️ Drift detected!")
else:
    print("✅ No significant drift.")

"""##  LightGBM vs XGBoost (Compare)
| Feature              | XGBoost | LightGBM            |
| -------------------- | ------- | ------------------- |
| Accuracy             | High    | Similar (or better) |
| Speed                | Slower  | 🚀 Faster           |
| Handles missing?     | ✅ Yes   | ✅ Yes               |
| Categorical encoding | Manual  | ✅ Native support    |

Use LightGBM for faster training with large datasets.
"""

#                    LGBMRegressor(LightBGM)
import lightgbm as lgb
lgb_model = lgb.LGBMRegressor()
lgb_model.fit(x_train, y_train)

print("LightBGM:", evaluate(lgb_model, x_test, y_test))    # best score than xgb

#Save the Model
import joblib
joblib.dump(lgb_model, "lgb_model.pkl")

"""## Ensemble Stacking
Combine 2 models (LightBGM, XGB)
"""

from sklearn.ensemble import StackingRegressor
from sklearn.linear_model import Ridge


stack = StackingRegressor(
    estimators=[
        ("lgb_model",lgb_model),
        ("xgb",xgb)
    ],
    final_estimator=Ridge()
)
stack.fit(x_train, y_train)

print("Stack:", evaluate(stack, x_test, y_test))     # slightly better than LightBGM

"""## 📈 Future Improvements

- Feature engineering: polynomial features, feature selection
- Hyperparameter tuning with Optuna or GridSearchCV
- Streamlit-based web app deployment

## Author
Hemant K  
📧 hemant777.karpe@gmail.com
🔗 [LinkedIn](https://www.linkedin.com/in/hemant-karpe)
"""