# -*- coding: utf-8 -*-
"""LoanApproval.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hyp39Obd9L6nPr-9Uqgo4UOntQzJ2iju

#  Day 1: Classification Basics + Dataset Setup
"""

#download data from kaggle
!pip install opendatasets

import pandas as pd
import numpy as np
import opendatasets as od
import seaborn as sns
import matplotlib.pyplot as plt
import sklearn
import joblib

# imputing missing values in num_cols
from sklearn.impute import SimpleImputer
# scaling numeric value in 1-0 range
from sklearn.preprocessing import MinMaxScaler,FunctionTransformer
# Combine the numerical and categorical pipelines
from sklearn.compose import ColumnTransformer

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

od.download("https://www.kaggle.com/datasets/taweilo/loan-approval-classification-data?select=loan_data.csv")
#hemantk777  721b095dacb7cfa6129768282921b8fb

df=pd.read_csv("/content/loan-approval-classification-data/loan_data.csv")

df.head()

df.shape

df.info()

df.describe()

df.columns

correlation=df.select_dtypes(include=['number']).corr()
correlation['loan_status']

df.isna().sum()

"""#  Day 2: Model Training (Random Forest + XGBoost)"""

# Handle missing values (basic for now)
df.dropna()

# Encode categoricals
df = pd.get_dummies(df, drop_first=True)

df.shape

x=df.drop("loan_status",axis=1)
y=df["loan_status"]

X_train, X_test, y_train, y_test = train_test_split(
    x, y, test_size=0.2, random_state=42, stratify=y)

rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)
rf_preds = rf.predict(X_test)

xgb = XGBClassifier(use_label_encoder=False, eval_metric="logloss")
xgb.fit(X_train, y_train)
xgb_preds = xgb.predict(X_test)

def evaluate(y_test, y_pred, name):
    print(f"\n{name} Evaluation")
    print("Accuracy:", accuracy_score(y_test, y_pred))
    print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
    print("Classification Report:\n", classification_report(y_test, y_pred))

evaluate(y_test, rf_preds, "Random Forest")
evaluate(y_test, xgb_preds, "XGBoost")

#Save the Best Model
##   joblib.dump(xgb, "xgb_model.pkl")

"""# Day 3: Handle Class Imbalance (Class Weights + SMOTE)"""

# Visualize Imbalance
sns.countplot(x="loan_status", data=df)
plt.title("Class Distribution")
plt.xlabel("Loan Status (1=Approved, 0=Rejected)")
#plt.savefig("Class Distribution.png", dpi=300, bbox_inches='tight')
plt.show()

print("Value Counts:\n", df["loan_status"].value_counts())

# Retrain with class_weight='balanced' (RandomForest)
rf_balanced = RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42)
rf_balanced.fit(X_train, y_train)
rfb_preds = rf_balanced.predict(X_test)

evaluate(y_test, rfb_preds, "Random Forest (Balanced)")

# Use SMOTE (Synthetic Minority Oversampling)
from imblearn.over_sampling import SMOTE

smote = SMOTE(random_state=42)
X_sm, y_sm = smote.fit_resample(X_train, y_train)

print("After SMOTE:", np.bincount(y_sm))

# Train XGBoost on SMOTE-balanced data:
xgb_sm = XGBClassifier(use_label_encoder=False, eval_metric="logloss")
xgb_sm.fit(X_sm, y_sm)
xgb_preds_sm = xgb_sm.predict(X_test)

evaluate(y_test, xgb_preds_sm, "XGBoost (SMOTE)")

#  Comparison of Models With and Without Balancing
evaluate(y_test, rf_preds, "Random Forest")
evaluate(y_test, xgb_preds, "XGBoost")
evaluate(y_test, rfb_preds, "Random Forest (Balanced)")    # use class_weight='balanced'
evaluate(y_test, xgb_preds_sm, "XGBoost (SMOTE)")          # use smote

from sklearn.metrics import ConfusionMatrixDisplay
ConfusionMatrixDisplay.from_estimator(xgb_sm, X_test, y_test)
plt.title("Confusion Matrix - XGBoost (SMOTE)")
#plt.savefig("Confusion Matrix - XGBoost (SMOTE).png", dpi=300, bbox_inches='tight')
plt.show()

#Save the SMOTE XGB model (best model)
##     joblib.dump(xgb_sm, "xgb_smote.pkl")

"""# Day 4: Hyperparameter Tuning with GridSearchCV"""

#  Import Required Tools
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier

# Define Parameter Grid for Random Forest
rf_params = {
    'n_estimators': [100, 200],
    'max_depth': [4, 6, 8],
    'min_samples_split': [2, 5],
    'min_samples_leaf': [1, 2]
}

# Define Parameter Grid for XGBoost
xgb_params = {
    'n_estimators': [100, 200],
    'max_depth': [3, 6],
    'learning_rate': [0.05, 0.1],
    'subsample': [0.8, 1]
}

#  Run GridSearchCV (Random Forest)
rf_grid = GridSearchCV(
    estimator=RandomForestClassifier(class_weight='balanced', random_state=42),
    param_grid=rf_params,
    cv=5,
    scoring='f1',
    n_jobs=-1,
    verbose=1
)

rf_grid.fit(X_train, y_train)
print("Best RF Params:", rf_grid.best_params_)

# Run GridSearchCV (XGBoost)
xgb_grid = GridSearchCV(
    estimator=XGBClassifier(use_label_encoder=False, eval_metric='logloss'),
    param_grid=xgb_params,
    cv=5,
    scoring='f1',
    n_jobs=-1,
    verbose=1
)

xgb_grid.fit(X_train, y_train)
print("Best XGB Params:", xgb_grid.best_params_)

# Evaluate Best RF Model
best_rf = rf_grid.best_estimator_
rf_preds = best_rf.predict(X_test)

evaluate(y_test, rf_preds, "Tuned Random Forest")

# Evaluate Best XGB Model
best_xgb = xgb_grid.best_estimator_
xgb_preds = best_xgb.predict(X_test)

evaluate(y_test, xgb_preds, "Tuned XGBoost")

#  Save the Tuned XGB model (best model)
#joblib.dump(xgb_sm, "xgb_Tuned.pkl")

""" # Day 5: SHAP + Feature Importance for Classification"""

# Native Feature Importance (Random Forest / XGBoost)
importances = best_xgb.feature_importances_           # For best_xgb from GridSearchCV
features = x.columns

# Create a DataFrame for sorting
feat_df = pd.DataFrame({
    'Feature': features,
    'Importance': importances })

# Sort and get top 10
top_feats = feat_df.sort_values(by="Importance", ascending=False).head(10)

# Plot
plt.figure(figsize=(8, 6))
sns.barplot(x="Importance", y="Feature", data=top_feats, palette="viridis")
plt.title("Top 10 Feature Importances (XGBoost)")
plt.tight_layout()
#plt.savefig("Top 10 Feature Importances (XGBoost).png", dpi=300, bbox_inches='tight')
plt.show()

# shap
import shap
explainer = shap.Explainer(best_xgb)
shap_values = explainer(X_test)

shap.plots.backend = "matplotlib"   # Use matplotlib backend for plots

shap.plots.beeswarm(shap_values, max_display=15,show=False) # Global importance
plt.title("SHAP Global Feature Importance")
plt.tight_layout()
#plt.savefig("shap_beeswarm_plot.png", dpi=300, bbox_inches='tight')
plt.show()

shap.plots.waterfall(shap_values[0], show=False)  # Single prediction breakdown
plt.title("SHAP Waterfall for 1 Prediction")
plt.tight_layout()
#plt.savefig("shap_waterfall_plot.png", dpi=300, bbox_inches='tight')
plt.show()

#  SHAP Summary Bar Plot
shap.plots.bar(shap_values,show=False)   #  average importance of each feature across all predictions.
plt.title(" SHAP Summary Bar Plot")
plt.tight_layout()
#plt.savefig("SHAP_Summary_Bar_Plot.png", dpi=300, bbox_inches='tight')
plt.show()

"""# Day 6: Finalization

# Loan Approval Prediction - ML Classifier

Predict loan approval status using RandomForest and XGBoost.  
Includes full pipeline + SHAP explainability.

## üìå Dataset
Kaggle: [Loan Approval Classification Data](https://www.kaggle.com/datasets/taweilo/loan-approval-classification-data)

## üß™ Models Used
- Random Forest
- XGBoost (tuned with GridSearchCV)

## üîç Explainability
- SHAP Beeswarm: Top influencing features
- SHAP Waterfall: Individual predictions

## üßº Workflow
1. Cleaned and encoded data
2. Handled class imbalance (SMOTE & class weights)
3. Tuned models using GridSearchCV
4. Saved model with `joblib`
5. Pushed to GitHub

## üìä Results
### üîç Tuned XGBoost Evaluation

| Metric     | Score |
|------------|-------|
| Accuracy   | 0.94  |
| Precision  | 0.89  |
| Recall     | 0.81  |
| F1-Score   | 0.85  |

#### üìä Confusion Matrix

|                | Predicted: 0 | Predicted: 1 |
|----------------|--------------|--------------|
| **Actual: 0**  | 6810         | 190          |
| **Actual: 1**  | 387          | 1613         |

## üîÆ Future Improvements
- Build a Streamlit web app for real-time predictions
- Add ROC-AUC and precision-recall evaluation
- Track data drift using evidently
- Try Optuna for advanced hyperparameter tuning
- Engineer new features from income, loan ratios
- Build a model card to describe ethical usage

## Author
Hemant K  
üìß hemant777.karpe@gmail.com
üîó [LinkedIn](https://www.linkedin.com/in/hemant-karpe)
"""